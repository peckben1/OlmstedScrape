{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB: Some old code is commented out above replacement code\n",
    "import flickrapi\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values have been removed and replaced with empty strings on the publicly posted version\n",
    "api_key = \"\"\n",
    "api_secret = \"\"\n",
    "user_id = \"\"\n",
    "file_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr = flickrapi.FlickrAPI(api_key, api_secret, format='parsed-json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling list of photosets uploaded by ONHS, limited to one page of results at a time\n",
    "# All API calls are preceded by a one-second sleep to ensure compliance with Flickr's 3600-per-hour call limit\n",
    "# All API calls will attempt two retries in the case of an error\n",
    "i = 3\n",
    "while i:\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "        sets = flickr.photosets.getList(user_id=user_id, page=1)\n",
    "    except:\n",
    "        i -= 1\n",
    "        continue\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the total number of pages\n",
    "pages = sets['photosets']['pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting results to DataFrame\n",
    "picframe = pd.DataFrame(sets['photosets']['photoset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the rest of the pages\n",
    "for j in range(2, (pages+1)):\n",
    "    i = 3\n",
    "    while i:\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            sets = flickr.photosets.getList(user_id=user_id, page=j)\n",
    "        except:\n",
    "            i -= 1\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    if i == 0:\n",
    "        print(f\"Sets call for page {j} exceeded max tries\")\n",
    "        continue\n",
    "    picframe = picframe.append(pd.DataFrame(sets['photosets']['photoset']), ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at the structure\n",
    "picframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was eliminating the _content dict wrapping the title, but this is now unnecessary\n",
    "#picframe['title'] = [title['_content'] for title in picframe['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use known formatting scheme of titles to extract Olmsted firm job number - altered to be more robust, as \n",
    "# length of job number and positioning of pound sign are slightly inconsistent\n",
    "#picframe['JobNum'] = [title[5:9] for title in picframe['title']]\n",
    "picframe['JobNum'] = [title['_content'].split(',')[0].split('#')[1].strip() if title['_content'].find('Job') != -1 else \"None\" for title in picframe['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to get set IDs of desired job numbers - jobs can be excluded by commenting out the respective line\n",
    "setIDs = picframe['id'][\n",
    "    (picframe['JobNum'] == '290') | \n",
    "    (picframe['JobNum'] == '296') | \n",
    "    (picframe['JobNum'] == '297') | \n",
    "    (picframe['JobNum'] == '330') | \n",
    "    (picframe['JobNum'] == '623') | \n",
    "    (picframe['JobNum'] == '676') | \n",
    "    (picframe['JobNum'] == '1242') | \n",
    "    (picframe['JobNum'] == '1327') | \n",
    "    (picframe['JobNum'] == '1341') | \n",
    "    (picframe['JobNum'] == '2260') | \n",
    "    (picframe['JobNum'] == '2379') | \n",
    "    (picframe['JobNum'] == '2820') | \n",
    "    (picframe['JobNum'] == '2821') | \n",
    "    (picframe['JobNum'] == '2822') | \n",
    "    (picframe['JobNum'] == '2823') | \n",
    "    (picframe['JobNum'] == '2824') | \n",
    "    (picframe['JobNum'] == '2825') | \n",
    "    (picframe['JobNum'] == '2826') | \n",
    "    (picframe['JobNum'] == '2827') | \n",
    "    (picframe['JobNum'] == '2828') | \n",
    "    (picframe['JobNum'] == '2829') | \n",
    "    (picframe['JobNum'] == '2830') | \n",
    "    (picframe['JobNum'] == '2831') | \n",
    "    (picframe['JobNum'] == '2832') | \n",
    "    (picframe['JobNum'] == '2833') | \n",
    "    (picframe['JobNum'] == '2834') | \n",
    "    (picframe['JobNum'] == '2835') | \n",
    "    (picframe['JobNum'] == '2836') | \n",
    "    (picframe['JobNum'] == '2837') | \n",
    "    (picframe['JobNum'] == '2838') | \n",
    "    (picframe['JobNum'] == '2839') | \n",
    "    (picframe['JobNum'] == '2840') | \n",
    "    (picframe['JobNum'] == '2841') | \n",
    "    (picframe['JobNum'] == '2842') | \n",
    "    (picframe['JobNum'] == '2843') |\n",
    "    (picframe['JobNum'] == '2844') | \n",
    "    (picframe['JobNum'] == '2845') |\n",
    "    (picframe['JobNum'] == '2846') |\n",
    "    (picframe['JobNum'] == '2847') |\n",
    "    (picframe['JobNum'] == '2848') |\n",
    "    (picframe['JobNum'] == '2849') |\n",
    "    (picframe['JobNum'] == '2901') |\n",
    "    (picframe['JobNum'] == '2914') |\n",
    "    (picframe['JobNum'] == '2922') |\n",
    "    (picframe['JobNum'] == '2931') |\n",
    "    (picframe['JobNum'] == '2956') |\n",
    "    (picframe['JobNum'] == '2961') |\n",
    "    (picframe['JobNum'] == '2961') |\n",
    "    (picframe['JobNum'] == '2964') |\n",
    "    (picframe['JobNum'] == '2966') |\n",
    "    (picframe['JobNum'] == '3035') |\n",
    "    (picframe['JobNum'] == '3086') |\n",
    "    (picframe['JobNum'] == '3297') |\n",
    "    (picframe['JobNum'] == '3346') |\n",
    "    (picframe['JobNum'] == '3358') |\n",
    "    (picframe['JobNum'] == '3604') |\n",
    "    (picframe['JobNum'] == '3643') |\n",
    "    (picframe['JobNum'] == '3706') |\n",
    "    (picframe['JobNum'] == '3749') |\n",
    "    (picframe['JobNum'] == '5358') |\n",
    "    (picframe['JobNum'] == '5518') |\n",
    "    (picframe['JobNum'] == '5507') |\n",
    "    (picframe['JobNum'] == '6487') |\n",
    "    (picframe['JobNum'] == '6570') |\n",
    "    (picframe['JobNum'] == '6574') |\n",
    "    (picframe['JobNum'] == '6790') |\n",
    "    (picframe['JobNum'] == '7002') |\n",
    "    (picframe['JobNum'] == '7022') |\n",
    "    (picframe['JobNum'] == '7054') |\n",
    "    (picframe['JobNum'] == '7290') |\n",
    "    (picframe['JobNum'] == '7308') |\n",
    "    (picframe['JobNum'] == '7458') |\n",
    "    (picframe['JobNum'] == '7714') |\n",
    "    (picframe['JobNum'] == '7956') |\n",
    "    (picframe['JobNum'] == '9299') |\n",
    "    (picframe['JobNum'] == '9300') |\n",
    "    (picframe['JobNum'] == '9451') |\n",
    "    (picframe['JobNum'] == '9970') |\n",
    "    (picframe['JobNum'] == '9992') |\n",
    "    (picframe['JobNum'] == '12034') |\n",
    "    (picframe['JobNum'] == '12044')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get photo list from each set and extract photo IDs, compiling list of all photoIDs. Sleep added to limit API load\n",
    "photoIDs = []\n",
    "for setID in setIDs:\n",
    "    i = 3\n",
    "    while i:\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            photos = flickr.photosets.getPhotos(api_key=api_key, photoset_id=setID, user_id=user_id)\n",
    "        except:\n",
    "            i -= 1\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    if i == 0:\n",
    "        print(f\"Set ID {setID} exceeded max tries\")\n",
    "        continue\n",
    "    photoList = photos['photoset']['photo']\n",
    "    for photo in photoList:\n",
    "        photoIDs.append(photo['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file sources from photos, selecting for size by position in size list. -1 is always the original file. \n",
    "# For loop and sleep added to limit load. \n",
    "#urlList = [flickr.photos.getSizes(api_key=api_key, photo_id=photoID)['sizes']['size'][0]['source'] for photoID in photoIDs]\n",
    "urlList = []\n",
    "for photoID in photoIDs:\n",
    "    i = 3\n",
    "    while i:\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            sizeDict = flickr.photos.getSizes(api_key=api_key, photo_id=photoID)\n",
    "        except:\n",
    "            i -= 1\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    if i == 0:\n",
    "        print(f\"Photo ID source call {photoID} exceeded max tries\")\n",
    "        continue\n",
    "    urlList.append(sizeDict['sizes']['size'][-1]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through sources, saving files named as the corresponding photo ID + .jpg\n",
    "j = 0\n",
    "while j < len(urlList):\n",
    "    i = 3\n",
    "    while i:\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            urllib.request.urlretrieve(urlList[j], f'{file_path}/{photoIDs[j]}.jpg')\n",
    "        except:\n",
    "            i -= 1\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    if i == 0:\n",
    "        print(f\"URL {urlList[j]} exceeded max tries, photo {photoIDs[j]} not retrieved\")\n",
    "        continue\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to create a metadata file...\n",
    "infoList = []\n",
    "\n",
    "# Pulling the info for each photo and getting the description content\n",
    "for photoID in photoIDs:\n",
    "    i = 3\n",
    "    while i:\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            info = flickr.photos.getInfo(api_key=api_key, photo_id=photoID)['photo']['description']['_content']\n",
    "        except:\n",
    "            i -= 1\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    if i == 0:\n",
    "        print(f\"Photo ID {photoID} info call exceeded max tries\")\n",
    "        continue\n",
    "\n",
    "    # Using known formatting of description to create metadata spreadsheet. Splitting info into lines, eliminating newlines at colons\n",
    "    info = info.replace(':\\n', ':')\n",
    "    info = info.split('\\n')\n",
    "\n",
    "    # Slicing off the attribution lines (proper attribution will be given, though!)\n",
    "    info = info[:-2]\n",
    "\n",
    "    # Turning each line into a key-value pair and adding to a dict, then appending dict to list\n",
    "    # Initializing dict with photoID as \"id\"\n",
    "    infoDict = {'id': photoID}\n",
    "    for inf in info:\n",
    "        inf = inf.split(':', 2)\n",
    "        if len(inf) < 2:\n",
    "            continue\n",
    "        infoDict[inf[0]] = inf[1]\n",
    "    infoList.append(infoDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting list of dictionaries to dataframe\n",
    "infoframe = pd.DataFrame(infoList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's output that as a .csv\n",
    "infoframe.to_csv(f'{file_path}/metadata.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
